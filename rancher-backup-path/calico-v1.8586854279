{"apiVersion":"management.cattle.io/v3","kind":"RkeAddon","metadata":{"creationTimestamp":"2020-06-25T21:39:38Z","generation":1,"labels":{"backupper.cattle.io/old-uid":"7da15eb6-6659-42c1-a371-50cc13f28761","cattle.io/creator":"norman"},"name":"calico-v1.8","namespace":"cattle-global-data","resourceVersion":"2341","selfLink":"/apis/management.cattle.io/v3/namespaces/cattle-global-data/rkeaddons/calico-v1.8"},"template":"\n{{if eq .RBACConfig \"rbac\"}}\n## start rbac here\n\nkind: ClusterRole\napiVersion: rbac.authorization.k8s.io/v1beta1\nmetadata:\n  name: calico-node\nrules:\n  - apiGroups: [\"\"]\n    resources:\n      - namespaces\n    verbs:\n      - get\n      - list\n      - watch\n  - apiGroups: [\"\"]\n    resources:\n      - pods/status\n    verbs:\n      - update\n  - apiGroups: [\"\"]\n    resources:\n      - pods\n    verbs:\n      - get\n      - list\n      - watch\n      - patch\n  - apiGroups: [\"\"]\n    resources:\n      - services\n    verbs:\n      - get\n  - apiGroups: [\"\"]\n    resources:\n      - endpoints\n    verbs:\n      - get\n  - apiGroups: [\"\"]\n    resources:\n      - nodes\n    verbs:\n      - get\n      - list\n      - update\n      - watch\n  - apiGroups: [\"extensions\"]\n    resources:\n      - networkpolicies\n    verbs:\n      - get\n      - list\n      - watch\n  - apiGroups: [\"networking.k8s.io\"]\n    resources:\n      - networkpolicies\n    verbs:\n      - watch\n      - list\n  - apiGroups: [\"crd.projectcalico.org\"]\n    resources:\n      - globalfelixconfigs\n      - felixconfigurations\n      - bgppeers\n      - globalbgpconfigs\n      - bgpconfigurations\n      - ippools\n      - globalnetworkpolicies\n      - globalnetworksets\n      - networkpolicies\n      - clusterinformations\n      - hostendpoints\n    verbs:\n      - create\n      - get\n      - list\n      - update\n      - watch\n---\napiVersion: rbac.authorization.k8s.io/v1beta1\nkind: ClusterRoleBinding\nmetadata:\n  name: calico-node\nroleRef:\n  apiGroup: rbac.authorization.k8s.io\n  kind: ClusterRole\n  name: calico-node\nsubjects:\n- kind: ServiceAccount\n  name: calico-node\n  namespace: kube-system\n- apiGroup: rbac.authorization.k8s.io\n  kind: Group\n  name: system:nodes\n{{end}}\n## end rbac here\n\n---\nkind: ConfigMap\napiVersion: v1\nmetadata:\n name: calico-config\n namespace: kube-system\ndata:\n # To enable Typha, set this to \"calico-typha\" *and* set a non-zero value for Typha replicas\n # below.  We recommend using Typha if you have more than 50 nodes. Above 100 nodes it is\n # essential.\n typha_service_name: \"none\"\n # The CNI network configuration to install on each node.\n cni_network_config: |-\n    {\n     \"name\": \"k8s-pod-network\",\n     \"cniVersion\": \"0.3.0\",\n     \"plugins\": [\n       {\n         \"type\": \"calico\",\n         \"log_level\": \"WARNING\",\n         \"datastore_type\": \"kubernetes\",\n         \"nodename\": \"__KUBERNETES_NODE_NAME__\",\n         \"mtu\": 1500,\n         \"ipam\": {\n           \"type\": \"host-local\",\n           \"subnet\": \"usePodCidr\"\n         },\n         \"policy\": {\n           \"type\": \"k8s\",\n           \"k8s_auth_token\": \"__SERVICEACCOUNT_TOKEN__\"\n         },\n         \"kubernetes\": {\n           \"k8s_api_root\": \"https://__KUBERNETES_SERVICE_HOST__:__KUBERNETES_SERVICE_PORT__\",\n           \"kubeconfig\": \"{{.KubeCfg}}\"\n         }\n       },\n       {\n         \"type\": \"portmap\",\n         \"snat\": true,\n         \"capabilities\": {\"portMappings\": true}\n       }\n     ]\n    }\n\n---\n\n# This manifest installs the calico/node container, as well\n# as the Calico CNI plugins and network config on\n# each master and worker node in a Kubernetes cluster.\nkind: DaemonSet\napiVersion: extensions/v1beta1\nmetadata:\n  name: calico-node\n  namespace: kube-system\n  labels:\n    k8s-app: calico-node\nspec:\n  selector:\n    matchLabels:\n      k8s-app: calico-node\n  updateStrategy:\n{{if .UpdateStrategy}}\n{{ toYaml .UpdateStrategy | indent 4}}\n{{else}}\n    type: RollingUpdate\n    rollingUpdate:\n      maxUnavailable: 1\n{{end}}\n  template:\n    metadata:\n      labels:\n        k8s-app: calico-node\n      annotations:\n        # This, along with the CriticalAddonsOnly toleration below,\n        # marks the pod as a critical add-on, ensuring it gets\n        # priority scheduling and that its resources are reserved\n        # if it ever gets evicted.\n        scheduler.alpha.kubernetes.io/critical-pod: ''\n    spec:\n      affinity:\n        nodeAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n            nodeSelectorTerms:\n              - matchExpressions:\n                - key: beta.kubernetes.io/os\n                  operator: NotIn\n                  values:\n                    - windows\n      hostNetwork: true\n{{if .NodeSelector}}\n      nodeSelector:\n      {{ range $k, $v := .NodeSelector }}\n        {{ $k }}: \"{{ $v }}\"\n      {{ end }}\n{{end}}\n      tolerations:\n        # Make sure calico/node gets scheduled on all nodes.\n        - effect: NoSchedule\n          operator: Exists\n        # Mark the pod as a critical add-on for rescheduling.\n        - key: CriticalAddonsOnly\n          operator: Exists\n        - effect: NoExecute\n          operator: Exists\n        - key: \"node-role.kubernetes.io/controlplane\"\n          operator: \"Exists\"\n          effect: \"NoSchedule\"\n        - key: \"node-role.kubernetes.io/etcd\"\n          operator: \"Exists\"\n          effect: \"NoExecute\"\n      serviceAccountName: calico-node\n      terminationGracePeriodSeconds: 0\n      containers:\n        # Runs calico/node container on each Kubernetes node.  This\n        # container programs network policy and routes on each\n        # host.\n        - name: calico-node\n          image: {{.NodeImage}}\n          env:\n            # Use Kubernetes API as the backing datastore.\n            - name: DATASTORE_TYPE\n              value: \"kubernetes\"\n            # Disable felix logging to file\n            - name: FELIX_LOGFILEPATH\n              value: \"none\"\n            # Disable felix logging for syslog\n            - name: FELIX_LOGSEVERITYSYS\n              value: \"\"\n            # Enable felix logging to stdout\n            - name: FELIX_LOGSEVERITYSCREEN\n              value: \"Warning\"\n            # Cluster type to identify the deployment type\n            - name: CLUSTER_TYPE\n              value: \"k8s,bgp\"\n            # Disable file logging so kubectl logs works.\n            - name: CALICO_DISABLE_FILE_LOGGING\n              value: \"true\"\n            # Set Felix endpoint to host default action to ACCEPT.\n            - name: FELIX_DEFAULTENDPOINTTOHOSTACTION\n              value: \"ACCEPT\"\n            # Disable IPV6 on Kubernetes.\n            - name: FELIX_IPV6SUPPORT\n              value: \"false\"\n            # Set MTU for tunnel device used if ipip is enabled\n            - name: FELIX_IPINIPMTU\n              value: \"1440\"\n            # Wait for the datastore.\n            - name: WAIT_FOR_DATASTORE\n              value: \"true\"\n            # The default IPv4 pool to create on startup if none exists. Pod IPs will be\n            # chosen from this range. Changing this value after installation will have\n            # no effect. This should fall within --cluster-cidr.\n            - name: CALICO_IPV4POOL_CIDR\n              value: \"{{.ClusterCIDR}}\"\n            # Enable IPIP\n            - name: CALICO_IPV4POOL_IPIP\n              value: \"Always\"\n            # Enable IP-in-IP within Felix.\n            - name: FELIX_IPINIPENABLED\n              value: \"true\"\n            # Typha support: controlled by the ConfigMap.\n            - name: FELIX_TYPHAK8SSERVICENAME\n              valueFrom:\n                configMapKeyRef:\n                  name: calico-config\n                  key: typha_service_name\n            # Set based on the k8s node name.\n            - name: NODENAME\n              valueFrom:\n                fieldRef:\n                  fieldPath: spec.nodeName\n            # Auto-detect the BGP IP address.\n            - name: IP\n              value: \"autodetect\"\n            - name: FELIX_HEALTHENABLED\n              value: \"true\"\n          securityContext:\n            privileged: true\n          resources:\n            requests:\n              cpu: 250m\n          livenessProbe:\n            httpGet:\n              path: /liveness\n              port: 9099\n            periodSeconds: 10\n            initialDelaySeconds: 10\n            failureThreshold: 6\n          readinessProbe:\n            httpGet:\n              path: /readiness\n              port: 9099\n            periodSeconds: 10\n          volumeMounts:\n            - mountPath: /lib/modules\n              name: lib-modules\n              readOnly: true\n            - mountPath: /var/run/calico\n              name: var-run-calico\n              readOnly: false\n            - mountPath: /var/lib/calico\n              name: var-lib-calico\n              readOnly: false\n        # This container installs the Calico CNI binaries\n        # and CNI network config file on each node.\n        - name: install-cni\n          image: {{.CNIImage}}\n          command: [\"/install-cni.sh\"]\n          env:\n            # Name of the CNI config file to create.\n            - name: CNI_CONF_NAME\n              value: \"10-calico.conflist\"\n            # The CNI network config to install on each node.\n            - name: CNI_NETWORK_CONFIG\n              valueFrom:\n                configMapKeyRef:\n                  name: calico-config\n                  key: cni_network_config\n            # Set the hostname based on the k8s node name.\n            - name: KUBERNETES_NODE_NAME\n              valueFrom:\n                fieldRef:\n                  fieldPath: spec.nodeName\n          volumeMounts:\n            - mountPath: /host/opt/cni/bin\n              name: cni-bin-dir\n            - mountPath: /host/etc/cni/net.d\n              name: cni-net-dir\n      volumes:\n        # Used by calico/node.\n        - name: lib-modules\n          hostPath:\n            path: /lib/modules\n        - name: var-run-calico\n          hostPath:\n            path: /var/run/calico\n        - name: var-lib-calico\n          hostPath:\n            path: /var/lib/calico\n        # Used to install CNI.\n        - name: cni-bin-dir\n          hostPath:\n            path: /opt/cni/bin\n        - name: cni-net-dir\n          hostPath:\n            path: /etc/cni/net.d\n\n# Create all the CustomResourceDefinitions needed for\n# Calico policy and networking mode.\n---\n\napiVersion: apiextensions.k8s.io/v1beta1\nkind: CustomResourceDefinition\nmetadata:\n   name: felixconfigurations.crd.projectcalico.org\nspec:\n  scope: Cluster\n  group: crd.projectcalico.org\n  version: v1\n  names:\n    kind: FelixConfiguration\n    plural: felixconfigurations\n    singular: felixconfiguration\n\n---\napiVersion: apiextensions.k8s.io/v1beta1\nkind: CustomResourceDefinition\nmetadata:\n  name: bgppeers.crd.projectcalico.org\nspec:\n  scope: Cluster\n  group: crd.projectcalico.org\n  version: v1\n  names:\n    kind: BGPPeer\n    plural: bgppeers\n    singular: bgppeer\n\n---\n\napiVersion: apiextensions.k8s.io/v1beta1\nkind: CustomResourceDefinition\nmetadata:\n  name: bgpconfigurations.crd.projectcalico.org\nspec:\n  scope: Cluster\n  group: crd.projectcalico.org\n  version: v1\n  names:\n    kind: BGPConfiguration\n    plural: bgpconfigurations\n    singular: bgpconfiguration\n\n---\n\napiVersion: apiextensions.k8s.io/v1beta1\nkind: CustomResourceDefinition\nmetadata:\n  name: ippools.crd.projectcalico.org\nspec:\n  scope: Cluster\n  group: crd.projectcalico.org\n  version: v1\n  names:\n    kind: IPPool\n    plural: ippools\n    singular: ippool\n\n---\n\napiVersion: apiextensions.k8s.io/v1beta1\nkind: CustomResourceDefinition\nmetadata:\n  name: hostendpoints.crd.projectcalico.org\nspec:\n  scope: Cluster\n  group: crd.projectcalico.org\n  version: v1\n  names:\n    kind: HostEndpoint\n    plural: hostendpoints\n    singular: hostendpoint\n\n---\n\napiVersion: apiextensions.k8s.io/v1beta1\nkind: CustomResourceDefinition\nmetadata:\n  name: clusterinformations.crd.projectcalico.org\nspec:\n  scope: Cluster\n  group: crd.projectcalico.org\n  version: v1\n  names:\n    kind: ClusterInformation\n    plural: clusterinformations\n    singular: clusterinformation\n\n---\n\napiVersion: apiextensions.k8s.io/v1beta1\nkind: CustomResourceDefinition\nmetadata:\n  name: globalnetworkpolicies.crd.projectcalico.org\nspec:\n  scope: Cluster\n  group: crd.projectcalico.org\n  version: v1\n  names:\n    kind: GlobalNetworkPolicy\n    plural: globalnetworkpolicies\n    singular: globalnetworkpolicy\n\n---\n\napiVersion: apiextensions.k8s.io/v1beta1\nkind: CustomResourceDefinition\nmetadata:\n  name: globalnetworksets.crd.projectcalico.org\nspec:\n  scope: Cluster\n  group: crd.projectcalico.org\n  version: v1\n  names:\n    kind: GlobalNetworkSet\n    plural: globalnetworksets\n    singular: globalnetworkset\n\n---\n\napiVersion: apiextensions.k8s.io/v1beta1\nkind: CustomResourceDefinition\nmetadata:\n  name: networkpolicies.crd.projectcalico.org\nspec:\n  scope: Namespaced\n  group: crd.projectcalico.org\n  version: v1\n  names:\n    kind: NetworkPolicy\n    plural: networkpolicies\n    singular: networkpolicy\n\n---\napiVersion: v1\nkind: ServiceAccount\nmetadata:\n  name: calico-node\n  namespace: kube-system\n\n\n{{if ne .CloudProvider \"none\"}}\n---\nkind: ConfigMap\napiVersion: v1\nmetadata:\n  name: {{.CloudProvider}}-ippool\n  namespace: kube-system\ndata:\n  {{.CloudProvider}}-ippool: |-\n    apiVersion: projectcalico.org/v3\n    kind: IPPool\n    metadata:\n      name: ippool-ipip-1\n    spec:\n      cidr: {{.ClusterCIDR}}\n      ipipMode: Always\n      natOutgoing: true\n---\napiVersion: v1\nkind: Pod\nmetadata:\n  name: calicoctl\n  namespace: kube-system\nspec:\n  hostNetwork: true\n  restartPolicy: OnFailure\n  tolerations:\n  - effect: NoExecute\n    operator: Exists\n  - effect: NoSchedule\n    operator: Exists\n  containers:\n  - name: calicoctl\n    image: {{.Calicoctl}}\n    command: [\"/bin/sh\", \"-c\", \"calicoctl apply -f {{.CloudProvider}}-ippool.yaml\"]\n    env:\n    - name: DATASTORE_TYPE\n      value: kubernetes\n    volumeMounts:\n    - name: ippool-config\n      mountPath: /root/\n  volumes:\n  - name: ippool-config\n    configMap:\n      name: {{.CloudProvider}}-ippool\n      items:\n        - key: {{.CloudProvider}}-ippool\n          path: {{.CloudProvider}}-ippool.yaml\n  # Mount in the etcd TLS secrets.\n{{end}}\n"}